{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Advisor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Import Trip Advisor data\n",
    "2. Tokenize the data (create a word index that represents words as numbers)\n",
    "3. Use an oov token to include words not seen before\n",
    "4. Pad the sentences to have similar length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Variables\n",
    "vocab_size = 10000\n",
    "trunc_type =\"post\"\n",
    "padding_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tripadvisor_hotel_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fabulous hotel mum just returned 4 night stay hotel 1898 fabulous, recently decorated rooms immaculate decor fantastic really beautiful hotel, location perfect nicer end la ramblas need close treat return tranquility hotel busy day sight seeing.the staff attentive polite spoke perfect english, price drinks bar expect little hard swallow having gorgeous course meal local resturant price gin tonics.although roof pool undergoing refurbishment stay basement pool surrounding facilities adequate,  ', \"romantic international ambience spent honeymoon melia caribe 23-30. plane landed torrential downpour soaked skin steps plane, rained 7 days just hot gorgeous, truly loved resort food people, management helpful needed courteous friendly, nightly shows fun casino, pools incredible beach beautiful, just short stroll resort swim deserted stretches beach wanted, took outback tour must-do tourists, islanders live visit mountains macou beach enjoy lunch siesta hammocks, buy souviniers tour rum 2/bottle jewelry 5-10, shop beach bargain big time, n't pay 1/3 asking price, phone room dinner reservations needed places, avg, salary hotel workers 150.00 month 12-14 hrs./day 25 days work 5 days, mind tip not required wait staff maids, appreciative, occasional topless ladies seen beach, cool observing french spanish german people, kyaking sailing paddle boats free, best vacation deal experienced just awesome, email,  \", 'great hotel location union square stay great 6 people total 3 rooms, room little different, enjoyed location close union square shopping just cable car away wharf, staff friendly helpful, definately stay returning sf,  ', \"pretty outside smelly inside beach beautiful pool pretty overcrowded 2 main restaurants goodlooking, big disappointment stinky rooms cigarette smoke given better room smell live kind smell humid uncirculated air bathrooms, pillows lumpy rock hard kind sized bed partially covered queensized sheets nights, expected inclusive mean exactly experience beaches included water sports, turns breezes fine print stipulates non-motorized water sports included, n't intend spend time room ok resort public areas quite nice, fun dancing pool twice day good entertainment night,  \", 'kids loved spent nights hotel wife kids christmas holidays, perfect, service great food little bit expensive compared mexican standards, good choice restaurants dine going classical mexican italian french chinese coffe.the hot pool central patio fantastic kids parents well.pros everytingcons luxury taxis hotel little expensive say 140 mxp 15 minutes drive versus 30-40 mxp ordinary taxis,  ']\n",
      "[5, 4, 4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "sentences = list(df['Review'])\n",
    "labels = list(df['Rating'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, random_state=0)\n",
    "\n",
    "print(X_train[:5])\n",
    "print(y_train[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words (bag of words) with an oov token\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 330    2 3534   11  196   67   18    9    2 5022  330  559  446   13\n",
      " 1117  429   97   28   53    2   15   99  731  197  257  616  107   90\n",
      "  939  157 5667    2  328   21 1312  785   29    8  662  598  467   99\n",
      "  152   73  123   43  221   35  209    1  184  497  383  396  308 2076\n",
      "   73 4847    1 3082  893   30 4341 3079    9 2090   30 1586  362  549\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "(15368, 531)\n"
     ]
    }
   ],
   "source": [
    "sequences_training = tokenizer.texts_to_sequences(X_train)\n",
    "max_length = int(np.median([len(x) for x in X_train]))\n",
    "padded_training = pad_sequences(\n",
    "    sequences_training, \n",
    "    padding=padding_type, \n",
    "    truncating=trunc_type,\n",
    "    maxlen=max_length\n",
    ")\n",
    "\n",
    "print(padded_training[0])\n",
    "print(padded_training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   65  218  216  432  767    2   16    2  436  107  239  289  387\n",
      "    7   13   35 2864   67   13 4626 1187 1381 1148    1  129   12  244\n",
      " 3959  173 5356    8   28   12   41 1477    3   10    4  538   25   33\n",
      "    1  352   12   35  320   28  391  167 4696 5018  434  103   57    8\n",
      " 1477   12    8  705 6889  517   13    6  112  496 1182   11 1528  341\n",
      "    3  642  194 1263    6    1 1148    1  806   21  512 3186   30   57\n",
      "   20 1157    4   25   39   25 5110  935 2864  248 1208  258  111  225\n",
      "    2   31   44   42  723   25 5870 1564   65    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "(5123, 531)\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "padded_test = pad_sequences(\n",
    "    sequences_test, \n",
    "    padding=padding_type, \n",
    "    truncating=trunc_type,\n",
    "    maxlen=max_length\n",
    ")\n",
    "\n",
    "print(padded_test[0])\n",
    "print(padded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "padded_training = np.array(padded_training)\n",
    "y_train = np.array(y_train)\n",
    "padded_test = np.array(padded_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation = 'softmax')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 531, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 125       \n",
      "=================================================================\n",
      "Total params: 160,533\n",
      "Trainable params: 160,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "481/481 - 2s - loss: 5.7033 - accuracy: 0.1217 - val_loss: 5.6985 - val_accuracy: 0.0472\n",
      "Epoch 2/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1158 - val_loss: 5.6985 - val_accuracy: 0.2040\n",
      "Epoch 3/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1104 - val_loss: 5.6985 - val_accuracy: 0.0951\n",
      "Epoch 4/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1117 - val_loss: 5.6985 - val_accuracy: 0.0824\n",
      "Epoch 5/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1095 - val_loss: 5.6985 - val_accuracy: 0.1052\n",
      "Epoch 6/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1128 - val_loss: 5.6985 - val_accuracy: 0.1893\n",
      "Epoch 7/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1129 - val_loss: 5.6985 - val_accuracy: 0.0519\n",
      "Epoch 8/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1085 - val_loss: 5.6985 - val_accuracy: 0.1290\n",
      "Epoch 9/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1067 - val_loss: 5.6985 - val_accuracy: 0.0902\n",
      "Epoch 10/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1100 - val_loss: 5.6985 - val_accuracy: 0.0262\n",
      "Epoch 11/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1083 - val_loss: 5.6985 - val_accuracy: 0.0621\n",
      "Epoch 12/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1114 - val_loss: 5.6985 - val_accuracy: 0.2007\n",
      "Epoch 13/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1126 - val_loss: 5.6985 - val_accuracy: 0.2171\n",
      "Epoch 14/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1101 - val_loss: 5.6985 - val_accuracy: 0.0484\n",
      "Epoch 15/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1109 - val_loss: 5.6985 - val_accuracy: 0.1200\n",
      "Epoch 16/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1061 - val_loss: 5.6985 - val_accuracy: 0.0082\n",
      "Epoch 17/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1106 - val_loss: 5.6985 - val_accuracy: 0.1491\n",
      "Epoch 18/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1092 - val_loss: 5.6985 - val_accuracy: 0.1048\n",
      "Epoch 19/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1132 - val_loss: 5.6985 - val_accuracy: 0.0316\n",
      "Epoch 20/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1051 - val_loss: 5.6985 - val_accuracy: 0.1804\n",
      "Epoch 21/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1106 - val_loss: 5.6985 - val_accuracy: 0.1927\n",
      "Epoch 22/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1113 - val_loss: 5.6985 - val_accuracy: 0.0221\n",
      "Epoch 23/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1080 - val_loss: 5.6985 - val_accuracy: 0.0601\n",
      "Epoch 24/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1147 - val_loss: 5.6985 - val_accuracy: 0.0185\n",
      "Epoch 25/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1056 - val_loss: 5.6985 - val_accuracy: 0.0576\n",
      "Epoch 26/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1153 - val_loss: 5.6985 - val_accuracy: 0.1854\n",
      "Epoch 27/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1104 - val_loss: 5.6985 - val_accuracy: 0.1597\n",
      "Epoch 28/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1130 - val_loss: 5.6985 - val_accuracy: 0.1007\n",
      "Epoch 29/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1109 - val_loss: 5.6985 - val_accuracy: 0.0759\n",
      "Epoch 30/30\n",
      "481/481 - 1s - loss: 5.7033 - accuracy: 0.1087 - val_loss: 5.6985 - val_accuracy: 0.0076\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(\n",
    "    padded_training, y_train,\n",
    "    epochs= num_epochs,\n",
    "    validation_data = (padded_test, y_test),\n",
    "    verbose=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
