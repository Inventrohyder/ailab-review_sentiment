{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Advisor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Import Trip Advisor data\n",
    "2. Tokenize the data (create a word index that represents words as numbers)\n",
    "3. Use an oov token to include words not seen before\n",
    "4. Pad the sentences to have similar length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Variables\n",
    "vocab_size = 10000\n",
    "trunc_type =\"post\"\n",
    "padding_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fabul hotel mum return 4 night stay hotel 1898...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>romant intern ambienc spent honeymoon melia ca...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>great hotel locat union squar stay great 6 peo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>pretti outsid smelli insid beach beauti pool p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kid love spent night hotel wife kid christma h...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review  Rating\n",
       "0           0  fabul hotel mum return 4 night stay hotel 1898...       4\n",
       "1           1  romant intern ambienc spent honeymoon melia ca...       4\n",
       "2           2  great hotel locat union squar stay great 6 peo...       3\n",
       "3           3  pretti outsid smelli insid beach beauti pool p...       1\n",
       "4           4  kid love spent night hotel wife kid christma h...       4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('to_model.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fabul hotel mum return 4 night stay hotel 1898 fabulous, recent decor room immacul decor fantast realli beauti hotel, locat perfect nicer end la rambla need close treat return tranquil hotel busi day sight seeing.th staff attent polit spoke perfect english, price drink bar expect littl hard swallow gorgeou cours meal local restur price gin tonics.although roof pool undergo refurbish stay basement pool surround facil adequate,', \"romant intern ambienc spent honeymoon melia carib 23-30. plane land torrenti downpour soak skin step plane, rain 7 day hot gorgeous, truli love resort food people, manag help need courteou friendly, nightli show fun casino, pool incred beach beautiful, short stroll resort swim desert stretch beach wanted, took outback tour must-do tourists, island live visit mountain macou beach enjoy lunch siesta hammocks, buy souvini tour rum 2/bottl jewelri 5-10, shop beach bargain big time, n't pay 1/3 ask price, phone room dinner reserv need places, avg, salari hotel worker 150.00 month 12-14 hrs./day 25 day work 5 days, mind tip requir wait staff maids, appreciative, occasion topless ladi seen beach, cool observ french spanish german people, kyak sail paddl boat free, best vacat deal experienc awesome, email,\", 'great hotel locat union squar stay great 6 peopl total 3 rooms, room littl different, enjoy locat close union squar shop cabl car away wharf, staff friendli helpful, defin stay return sf,', \"pretti outsid smelli insid beach beauti pool pretti overcrowd 2 main restaur goodlooking, big disappoint stinki room cigarett smoke given better room smell live kind smell humid uncircul air bathrooms, pillow lumpi rock hard kind size bed partial cover queensiz sheet nights, expect inclus mean exactli experi beach includ water sports, turn breez fine print stipul non-motor water sport included, n't intend spend time room ok resort public area quit nice, fun danc pool twice day good entertain night,\", 'kid love spent night hotel wife kid christma holidays, perfect, servic great food littl bit expens compar mexican standards, good choic restaur dine go classic mexican italian french chines coffe.th hot pool central patio fantast kid parent well.pro everytingcon luxuri taxi hotel littl expens say 140 mxp 15 minut drive versu 30-40 mxp ordinari taxis,']\n",
      "[4, 3, 3, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "X_train = list(df['Review'])\n",
    "y_train = list(df['Rating'])\n",
    "\n",
    "print(X_train[:5])\n",
    "print(y_train[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ehhh better punta cana twice compar hotel stay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4 n't think, decid book atenea night stay dece...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>awesom time return vacat fantast time, weather...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>grand oasi wonder second time, group 20 friend...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bad stay stay hotel famili attend javaon 2008....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review  Rating\n",
       "0           0  ehhh better punta cana twice compar hotel stay...       1\n",
       "1           1  4 n't think, decid book atenea night stay dece...       1\n",
       "2           2  awesom time return vacat fantast time, weather...       3\n",
       "3           3  grand oasi wonder second time, group 20 friend...       4\n",
       "4           4  bad stay stay hotel famili attend javaon 2008....       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ehhh better punta cana twice compar hotel stay hotel major need help, let start good room littl hous 4 room bungalow meant privacy, minibar minifrig pretti nice includ array differ liquors, staff realli nice help exception, room smell like peopl expierience, pool nice, littl shop realli cool club play mixtur music, bad, desk staff except nice staff, complet unorgan rude, room n't use key card keys, charg room prepar pay cash, n't creditcards, minibar frige stock day, worst hike pool desk beach, closer like, area like maze lost hous look map given, overal rate hotel 2-3 5 stars, like titl state better,\", \"4 n't think, decid book atenea night stay decemb receiv fairli good review conveni metro provid breakfast, unfortun disappoint ye hotel conveni metro posit feature, hotel date curtain carpet room bit scrub, room kitchen unless bring kitchen utensil major advantage, plu sheet clean provid clean towel day, wall hotel extrem familiar neighbour nocturn habit smell smoke rooms.breakfast bizarr affair grant stay breakfast serv differ locat norm excus complet lack organisation, room consist mainli larg tabl seat 8 peopl share groups, morn wait bowl wash cereal morn cereal soggi time got spoons, toaster work hot food luke warm.barcelona great citi plenti offer visit certainli stay hotel atenea,\", 'awesom time return vacat fantast time, weather realli nice ground immaculate, dominicain friendliest peopl meet, plan trip palladium go shortli like tip advic restaur best send email happi help, reach hermosoazulojo yahoo.ca, hope hear,', 'grand oasi wonder second time, group 20 friend went grand oasi april 5-12 fantast time, husband januari plan wed friend group, wed awesome, jose pr person arrang marri coupl beach planned, champang beach wed dinner damario italian housekeep staff decor room, entir staff watch littl thing right, ate restaur loved, friday night given special dinner beach appl vacat steak grill shrimp bbq chicken trimmings, say nice thing resort, wonder job gm pablo carin jose registr staff wait staff housekeep guy sweep seawe morning, plan go januari april 2009. husband say new home,', 'bad stay stay hotel famili attend javaon 2008. lot posit hotel disappointments.posit 1- great location, easi 80 head oakland easi water easi 101 head silicon valley, nice walk moscon center 2- sleep 5 comfort room 2 queen sofa-b 3- great price, hotel area good value, 4- help desk staffdisappoint 1- poll small, deepest end 3.5 feet ye littl meter, shallow end 3 feet 2- park expensive, 50/night includ tax 3- 2 5 night housekeep sheet sofa-bed, time took 3 hour arriv 4- day arriv street main entranc close make difficult hotel,']\n",
      "[1, 4, 4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "X_test = list(df['Review'])\n",
    "y_test = list(df['Rating'])\n",
    "\n",
    "print(X_test[:5])\n",
    "print(y_test[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words (bag of words) with an oov token\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 413    2 3188   60   75    9    4    2 4613 1177  420  219    3 1059\n",
      "  219  147   25   64    2   13  105  751  133  189  537   58   82  421\n",
      "   60 2811    2  134   11  620 3097   27    8  471  626  475  105  158\n",
      "   51   69   30   93   36  204 8673  660  400  263  253 1154   51 4396\n",
      "    1 2699  858   24 3575 1452    4 1802   24  753  356 1453    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "(15368, 470)\n"
     ]
    }
   ],
   "source": [
    "sequences_training = tokenizer.texts_to_sequences(X_train)\n",
    "max_length = int(np.median([len(x) for x in X_train]))\n",
    "padded_training = pad_sequences(\n",
    "    sequences_training, \n",
    "    padding=padding_type, \n",
    "    truncating=trunc_type,\n",
    "    maxlen=max_length\n",
    ")\n",
    "\n",
    "print(padded_training[0])\n",
    "print(padded_training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   72  211  209  439  474    2    4    2  371   58   35  265  214\n",
      "    7    3   36  422   75    3 3413 1084 2869 1032    1  145   10  114\n",
      " 3527  154    1    8   25   10   35 3929    3  337   21   40    1   24\n",
      "   10   36   87   25  364  160  408 4686  433  113   62    8  585   10\n",
      "    8  398 6720  508    3    6   53  401  323 4650  237    3  588  139\n",
      " 1157    6    1 1032    1  730   11  543 2256   24   62   15 1078   21\n",
      "   29   21 4605  884  422   45  876  264  127  130    2   28   50   46\n",
      " 1669   21 4696  581   72    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "(5123, 470)\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "padded_test = pad_sequences(\n",
    "    sequences_test, \n",
    "    padding=padding_type, \n",
    "    truncating=trunc_type,\n",
    "    maxlen=max_length\n",
    ")\n",
    "\n",
    "print(padded_test[0])\n",
    "print(padded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "padded_training = np.array(padded_training)\n",
    "y_train = np.array(y_train)\n",
    "padded_test = np.array(padded_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 470, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 160,709\n",
      "Trainable params: 160,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "481/481 - 1s - loss: 1.3827 - accuracy: 0.4323 - val_loss: 1.3355 - val_accuracy: 0.4353\n",
      "Epoch 2/30\n",
      "481/481 - 1s - loss: 1.2622 - accuracy: 0.4541 - val_loss: 1.1440 - val_accuracy: 0.4650\n",
      "Epoch 3/30\n",
      "481/481 - 1s - loss: 1.0537 - accuracy: 0.5134 - val_loss: 1.0415 - val_accuracy: 0.5366\n",
      "Epoch 4/30\n",
      "481/481 - 1s - loss: 0.9703 - accuracy: 0.5499 - val_loss: 0.9915 - val_accuracy: 0.5505\n",
      "Epoch 5/30\n",
      "481/481 - 1s - loss: 0.9280 - accuracy: 0.5696 - val_loss: 0.9667 - val_accuracy: 0.5698\n",
      "Epoch 6/30\n",
      "481/481 - 1s - loss: 0.8931 - accuracy: 0.5932 - val_loss: 0.9636 - val_accuracy: 0.5659\n",
      "Epoch 7/30\n",
      "481/481 - 1s - loss: 0.8637 - accuracy: 0.6022 - val_loss: 0.9452 - val_accuracy: 0.5846\n",
      "Epoch 8/30\n",
      "481/481 - 1s - loss: 0.8372 - accuracy: 0.6208 - val_loss: 0.9294 - val_accuracy: 0.5944\n",
      "Epoch 9/30\n",
      "481/481 - 1s - loss: 0.8069 - accuracy: 0.6378 - val_loss: 0.9252 - val_accuracy: 0.6006\n",
      "Epoch 10/30\n",
      "481/481 - 1s - loss: 0.7855 - accuracy: 0.6498 - val_loss: 0.9242 - val_accuracy: 0.6032\n",
      "Epoch 11/30\n",
      "481/481 - 1s - loss: 0.7635 - accuracy: 0.6650 - val_loss: 0.9245 - val_accuracy: 0.6059\n",
      "Epoch 12/30\n",
      "481/481 - 1s - loss: 0.7460 - accuracy: 0.6696 - val_loss: 0.9243 - val_accuracy: 0.6129\n",
      "Epoch 13/30\n",
      "481/481 - 1s - loss: 0.7249 - accuracy: 0.6864 - val_loss: 0.9216 - val_accuracy: 0.6160\n",
      "Epoch 14/30\n",
      "481/481 - 1s - loss: 0.7055 - accuracy: 0.6922 - val_loss: 0.9295 - val_accuracy: 0.6036\n",
      "Epoch 15/30\n",
      "481/481 - 1s - loss: 0.6864 - accuracy: 0.7061 - val_loss: 0.9320 - val_accuracy: 0.6160\n",
      "Epoch 16/30\n",
      "481/481 - 1s - loss: 0.6716 - accuracy: 0.7113 - val_loss: 0.9342 - val_accuracy: 0.6166\n",
      "Epoch 17/30\n",
      "481/481 - 1s - loss: 0.6507 - accuracy: 0.7235 - val_loss: 0.9451 - val_accuracy: 0.6084\n",
      "Epoch 18/30\n",
      "481/481 - 1s - loss: 0.6387 - accuracy: 0.7334 - val_loss: 0.9573 - val_accuracy: 0.6049\n",
      "Epoch 19/30\n",
      "481/481 - 1s - loss: 0.6187 - accuracy: 0.7432 - val_loss: 0.9704 - val_accuracy: 0.6112\n",
      "Epoch 20/30\n",
      "481/481 - 1s - loss: 0.6035 - accuracy: 0.7483 - val_loss: 0.9874 - val_accuracy: 0.6133\n",
      "Epoch 21/30\n",
      "481/481 - 1s - loss: 0.5892 - accuracy: 0.7576 - val_loss: 0.9958 - val_accuracy: 0.6061\n",
      "Epoch 22/30\n",
      "481/481 - 1s - loss: 0.5740 - accuracy: 0.7685 - val_loss: 1.0169 - val_accuracy: 0.6049\n",
      "Epoch 23/30\n",
      "481/481 - 1s - loss: 0.5620 - accuracy: 0.7708 - val_loss: 1.0297 - val_accuracy: 0.6022\n",
      "Epoch 24/30\n",
      "481/481 - 1s - loss: 0.5438 - accuracy: 0.7855 - val_loss: 1.0487 - val_accuracy: 0.6055\n",
      "Epoch 25/30\n",
      "481/481 - 1s - loss: 0.5311 - accuracy: 0.7897 - val_loss: 1.0648 - val_accuracy: 0.6037\n",
      "Epoch 26/30\n",
      "481/481 - 1s - loss: 0.5176 - accuracy: 0.7959 - val_loss: 1.0852 - val_accuracy: 0.6039\n",
      "Epoch 27/30\n",
      "481/481 - 1s - loss: 0.5045 - accuracy: 0.8018 - val_loss: 1.1194 - val_accuracy: 0.6010\n",
      "Epoch 28/30\n",
      "481/481 - 1s - loss: 0.4921 - accuracy: 0.8074 - val_loss: 1.1373 - val_accuracy: 0.6069\n",
      "Epoch 29/30\n",
      "481/481 - 1s - loss: 0.4763 - accuracy: 0.8166 - val_loss: 1.1689 - val_accuracy: 0.6016\n",
      "Epoch 30/30\n",
      "481/481 - 1s - loss: 0.4651 - accuracy: 0.8237 - val_loss: 1.1940 - val_accuracy: 0.6002\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(\n",
    "    padded_training, y_train,\n",
    "    epochs= num_epochs,\n",
    "    validation_data = (padded_test, y_test),\n",
    "    verbose=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
