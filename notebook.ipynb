{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Advisor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Import Trip Advisor data\n",
    "2. Tokenize the data (create a word index that represents words as numbers)\n",
    "3. Use an oov token to include words not seen before\n",
    "4. Pad the sentences to have similar length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Variables\n",
    "vocab_size = 10000\n",
    "trunc_type =\"post\"\n",
    "padding_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6375</td>\n",
       "      <td>great place stay, great place stay looking hot...</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11040</td>\n",
       "      <td>nice hotel overal overall inn opera boutique h...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>great hotel location union square stay great 6...</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4636</td>\n",
       "      <td>nice hotel great location nice hotel great loc...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2243</td>\n",
       "      <td>dissapointing 5 star hotel just got 6 night st...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review  Rating  \\\n",
       "0        6375  great place stay, great place stay looking hot...       3   \n",
       "1       11040  nice hotel overal overall inn opera boutique h...       1   \n",
       "2           2  great hotel location union square stay great 6...       3   \n",
       "3        4636  nice hotel great location nice hotel great loc...       4   \n",
       "4        2243  dissapointing 5 star hotel just got 6 night st...       1   \n",
       "\n",
       "   weight  \n",
       "0      21  \n",
       "1      29  \n",
       "2      21  \n",
       "3      14  \n",
       "4      29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('to_model.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great place stay, great place stay looking hotel close airport, room clean staff helpful breakfast good shuttle great, booked reservation upcoming trip, a+,  ', 'nice hotel overal overall inn opera boutique hotel nice antique furnishings small, beware noise issues, heard plumbing surrounding rooms toilet flushes water pressure pipes running shower/bath etc., noise got quite audible odd hours, got home night room true oasis, hope room works, nice quiet non-union square locale relaxing,  ', 'great hotel location union square stay great 6 people total 3 rooms, room little different, enjoyed location close union square shopping just cable car away wharf, staff friendly helpful, definately stay returning sf,  ', \"nice hotel great location nice hotel great location arno river, rooms large bathroom large compared places stayed italy a/c really worked, staff helpful accomodating maps directions restaurant recommendations taxis laundry service, booth hotel lobby internet access, n't miss gelato vivolo near san croce,  \", 'dissapointing 5 star hotel just got 6 night stay, originally chose hotel location good rate given.however went check noticed rate actually higher told, add extra tax dont know bill.dont believe pictures website rooms look nothing like, given room 56th floor promised riverview instead got views building work suntec city quarter river, times little ants crawling round near bedside cabinet apart room clean, expected better 5 star dissapointed, reception staff not helpful, breakfast average choice.the best hotel excellent location city hall mrt right doorstep suntec city mins walk.overall wouldnt stay money stay better hotel, rate hotel average 3 star,  ']\n",
      "[4, 0, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "X_train = list(df['Review'])\n",
    "y_train = list(df['Rating'])\n",
    "\n",
    "print(X_train[:5])\n",
    "print(y_train[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ehhh better punta cana twice compared hotel st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4 n't think, decided book atenea night stay de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>awesome time just returned vacation fantastic ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>grand oasis wonderful second time, group 20 fr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>not bad stay stayed hotel family attending jav...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review  Rating\n",
       "0           0  ehhh better punta cana twice compared hotel st...       1\n",
       "1           1  4 n't think, decided book atenea night stay de...       1\n",
       "2           2  awesome time just returned vacation fantastic ...       3\n",
       "3           3  grand oasis wonderful second time, group 20 fr...       4\n",
       "4           4  not bad stay stayed hotel family attending jav...       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ehhh better punta cana twice compared hotel stayed hotel major need help, let start good rooms little houses 4 rooms bungalow meant privacy, minibar minifrige pretty nice included array different liquors, staff really nice helpful exception, room did not smell like people expierience, pools nice, little shops really cool club plays mixture music, bad, desk staff exception nice staff, completely unorganized rude, rooms n't use key cards just keys, charge room prepared pay cash, n't creditcards, minibar frige stocked day, worst hike pool desk beach, closer not like, area like maze lost houses look map given, overall rate hotel 2-3 5 stars, like title states better,  \", \"4 n't think, decided book atenea night stay december received fairly good reviews convenient metro provided breakfast, unfortunately disappointed yes hotel convenient metro positive feature, hotel dated curtains carpets rooms bit scrub, rooms kitchens unless bring kitchen utensils not major advantage, plus sheets clean provided clean towels day, walls hotel extremely familiar neighbours nocturnal habits smell smoke rooms.breakfast bizarre affair granted stay breakfast served different location norm did not excuse complete lack organisation, room consisted mainly large tables seated 8 people share groups, morning wait bowls washed cereal morning cereal soggy time got spoons, toaster did not work hot food luke warm.barcelona great city plenty offer visit certainly not stay hotel atenea,  \", 'awesome time just returned vacation fantastic time, weather really nice grounds immaculate, dominicains friendliest people meet, planned trip palladium going shortly like tips advice restaurants best send email happy help, reach hermosoazulojos yahoo.ca, hope hear,  ', 'grand oasis wonderful second time, group 20 friends went grand oasis april 5-12 fantastic time, husband january planned wedding friends group, wedding awesome, jose pr person arrangements married couple beach planned, champange beach wedding dinner damarios italian housekeeping staff decorated room, entire staff watches little thing right, ate restaurants loved, friday night given special dinner beach apple vacations steak grilled shrimp bbq chicken trimmings, not say nice things resort, does wonderful job gm pablo carin jose registration staff wait staff housekeeping guys sweeping seaweed morning, plan going january april 2009. husband says new home,  ', 'not bad stay stayed hotel family attending javaone 2008. lot positives hotel disappointments.positives 1- great location, easy 80 head oakland easy water easy 101 head silicon valley, nice walk moscone center 2- sleep 5 comfortable room 2 queens sofa-bed 3- great price, hotels area good value, 4- helpful desk staffdisappointments 1- poll small, deepest end 3.5 feet yes just little meter, shallow end 3 feet 2- parking expensive, 50/night include taxes 3- 2 5 nights housekeeping sheets sofa-bed, time took 3 hours arrive 4- day arrived street main entrance closed making difficult hotel,  ']\n",
      "[1, 4, 4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "X_test = list(df['Review'])\n",
    "y_test = list(df['Rating'])\n",
    "\n",
    "print(X_test[:5])\n",
    "print(y_test[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words (bag of words) with an oov token\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5   28    9    5   28    9  151    2  100  108    3   24    8   44\n",
      "   23    7  368    5   73  305 7068   49  289    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "(4610, 542)\n"
     ]
    }
   ],
   "source": [
    "sequences_training = tokenizer.texts_to_sequences(X_train)\n",
    "max_length = int(np.median([len(x) for x in X_train]))\n",
    "padded_training = pad_sequences(\n",
    "    sequences_training, \n",
    "    padding=padding_type, \n",
    "    truncating=trunc_type,\n",
    "    maxlen=max_length\n",
    ")\n",
    "\n",
    "print(padded_training[0])\n",
    "print(padded_training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   62  221  210  426  689    2   19    2  399  117  248  303  413\n",
      "    7   13   36 3467   60   13 3840 1121 1353 1081    1   98   12  253\n",
      " 3838  187 3832    8   27   12   44 1393    3   10    4  442   26   34\n",
      "    1  364   12   36  378   27  420  186 3893 3909  454   90   53    8\n",
      " 1393   12    8  684 4467  453   13    6   97  418 1242   14 1515  308\n",
      "    3  637  168 1403    6    1 1081    1  740   18  482 3270   30   53\n",
      "   17 1027    4   26   37   26 4844  910 3467  251 1305  247  115  240\n",
      "    2   31   40   39  645   26 6682 1582   62    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "(5123, 542)\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "padded_test = pad_sequences(\n",
    "    sequences_test, \n",
    "    padding=padding_type, \n",
    "    truncating=trunc_type,\n",
    "    maxlen=max_length\n",
    ")\n",
    "\n",
    "print(padded_test[0])\n",
    "print(padded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "import numpy as np\n",
    "padded_training = np.array(padded_training)\n",
    "y_train = np.array(y_train)\n",
    "padded_test = np.array(padded_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 542, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 160,709\n",
      "Trainable params: 160,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "145/145 - 1s - loss: 1.5152 - accuracy: 0.3384 - val_loss: 1.3758 - val_accuracy: 0.4353\n",
      "Epoch 2/50\n",
      "145/145 - 1s - loss: 1.4653 - accuracy: 0.3375 - val_loss: 1.3694 - val_accuracy: 0.4392\n",
      "Epoch 3/50\n",
      "145/145 - 1s - loss: 1.4552 - accuracy: 0.3382 - val_loss: 1.3541 - val_accuracy: 0.4408\n",
      "Epoch 4/50\n",
      "145/145 - 0s - loss: 1.4187 - accuracy: 0.3531 - val_loss: 1.3135 - val_accuracy: 0.4533\n",
      "Epoch 5/50\n",
      "145/145 - 0s - loss: 1.3287 - accuracy: 0.3950 - val_loss: 1.2172 - val_accuracy: 0.5017\n",
      "Epoch 6/50\n",
      "145/145 - 0s - loss: 1.1991 - accuracy: 0.4490 - val_loss: 1.1125 - val_accuracy: 0.5120\n",
      "Epoch 7/50\n",
      "145/145 - 0s - loss: 1.1054 - accuracy: 0.4777 - val_loss: 1.0589 - val_accuracy: 0.5245\n",
      "Epoch 8/50\n",
      "145/145 - 0s - loss: 1.0435 - accuracy: 0.5082 - val_loss: 1.0595 - val_accuracy: 0.5161\n",
      "Epoch 9/50\n",
      "145/145 - 0s - loss: 0.9958 - accuracy: 0.5343 - val_loss: 1.0057 - val_accuracy: 0.5432\n",
      "Epoch 10/50\n",
      "145/145 - 0s - loss: 0.9652 - accuracy: 0.5460 - val_loss: 0.9916 - val_accuracy: 0.5530\n",
      "Epoch 11/50\n",
      "145/145 - 0s - loss: 0.9235 - accuracy: 0.5779 - val_loss: 0.9724 - val_accuracy: 0.5669\n",
      "Epoch 12/50\n",
      "145/145 - 0s - loss: 0.8896 - accuracy: 0.6030 - val_loss: 0.9754 - val_accuracy: 0.5575\n",
      "Epoch 13/50\n",
      "145/145 - 0s - loss: 0.8622 - accuracy: 0.6191 - val_loss: 0.9686 - val_accuracy: 0.5674\n",
      "Epoch 14/50\n",
      "145/145 - 0s - loss: 0.8342 - accuracy: 0.6269 - val_loss: 0.9533 - val_accuracy: 0.5711\n",
      "Epoch 15/50\n",
      "145/145 - 0s - loss: 0.8071 - accuracy: 0.6460 - val_loss: 0.9683 - val_accuracy: 0.5715\n",
      "Epoch 16/50\n",
      "145/145 - 0s - loss: 0.7815 - accuracy: 0.6672 - val_loss: 0.9560 - val_accuracy: 0.5770\n",
      "Epoch 17/50\n",
      "145/145 - 0s - loss: 0.7570 - accuracy: 0.6748 - val_loss: 0.9649 - val_accuracy: 0.5819\n",
      "Epoch 18/50\n",
      "145/145 - 1s - loss: 0.7355 - accuracy: 0.6907 - val_loss: 0.9447 - val_accuracy: 0.5913\n",
      "Epoch 19/50\n",
      "145/145 - 0s - loss: 0.7123 - accuracy: 0.7106 - val_loss: 0.9561 - val_accuracy: 0.5854\n",
      "Epoch 20/50\n",
      "145/145 - 1s - loss: 0.6926 - accuracy: 0.7217 - val_loss: 0.9507 - val_accuracy: 0.5981\n",
      "Epoch 21/50\n",
      "145/145 - 0s - loss: 0.6702 - accuracy: 0.7302 - val_loss: 0.9678 - val_accuracy: 0.5938\n",
      "Epoch 22/50\n",
      "145/145 - 0s - loss: 0.6534 - accuracy: 0.7334 - val_loss: 0.9584 - val_accuracy: 0.5985\n",
      "Epoch 23/50\n",
      "145/145 - 0s - loss: 0.6255 - accuracy: 0.7510 - val_loss: 0.9807 - val_accuracy: 0.5893\n",
      "Epoch 24/50\n",
      "145/145 - 0s - loss: 0.6078 - accuracy: 0.7677 - val_loss: 0.9999 - val_accuracy: 0.5850\n",
      "Epoch 25/50\n",
      "145/145 - 0s - loss: 0.5891 - accuracy: 0.7787 - val_loss: 0.9746 - val_accuracy: 0.6045\n",
      "Epoch 26/50\n",
      "145/145 - 0s - loss: 0.5736 - accuracy: 0.7805 - val_loss: 0.9954 - val_accuracy: 0.6004\n",
      "Epoch 27/50\n",
      "145/145 - 0s - loss: 0.5478 - accuracy: 0.7941 - val_loss: 1.0078 - val_accuracy: 0.5995\n",
      "Epoch 28/50\n",
      "145/145 - 0s - loss: 0.5369 - accuracy: 0.8004 - val_loss: 1.0132 - val_accuracy: 0.6039\n",
      "Epoch 29/50\n",
      "145/145 - 0s - loss: 0.5145 - accuracy: 0.8143 - val_loss: 1.0217 - val_accuracy: 0.6096\n",
      "Epoch 30/50\n",
      "145/145 - 0s - loss: 0.4916 - accuracy: 0.8280 - val_loss: 1.0763 - val_accuracy: 0.5809\n",
      "Epoch 31/50\n",
      "145/145 - 0s - loss: 0.4766 - accuracy: 0.8356 - val_loss: 1.0617 - val_accuracy: 0.5985\n",
      "Epoch 32/50\n",
      "145/145 - 0s - loss: 0.4585 - accuracy: 0.8486 - val_loss: 1.1104 - val_accuracy: 0.5817\n",
      "Epoch 33/50\n",
      "145/145 - 0s - loss: 0.4463 - accuracy: 0.8505 - val_loss: 1.1222 - val_accuracy: 0.5856\n",
      "Epoch 34/50\n",
      "145/145 - 0s - loss: 0.4343 - accuracy: 0.8540 - val_loss: 1.1060 - val_accuracy: 0.5991\n",
      "Epoch 35/50\n",
      "145/145 - 0s - loss: 0.4187 - accuracy: 0.8614 - val_loss: 1.1187 - val_accuracy: 0.6026\n",
      "Epoch 36/50\n",
      "145/145 - 0s - loss: 0.3999 - accuracy: 0.8703 - val_loss: 1.1676 - val_accuracy: 0.5868\n",
      "Epoch 37/50\n",
      "145/145 - 0s - loss: 0.3849 - accuracy: 0.8790 - val_loss: 1.2149 - val_accuracy: 0.5735\n",
      "Epoch 38/50\n",
      "145/145 - 0s - loss: 0.3727 - accuracy: 0.8824 - val_loss: 1.1999 - val_accuracy: 0.5897\n",
      "Epoch 39/50\n",
      "145/145 - 0s - loss: 0.3613 - accuracy: 0.8859 - val_loss: 1.2031 - val_accuracy: 0.5954\n",
      "Epoch 40/50\n",
      "145/145 - 1s - loss: 0.3406 - accuracy: 0.8928 - val_loss: 1.2555 - val_accuracy: 0.5866\n",
      "Epoch 41/50\n",
      "145/145 - 0s - loss: 0.3331 - accuracy: 0.8954 - val_loss: 1.2764 - val_accuracy: 0.5838\n",
      "Epoch 42/50\n",
      "145/145 - 0s - loss: 0.3136 - accuracy: 0.9082 - val_loss: 1.2815 - val_accuracy: 0.5903\n",
      "Epoch 43/50\n",
      "145/145 - 0s - loss: 0.3074 - accuracy: 0.9085 - val_loss: 1.2839 - val_accuracy: 0.5967\n",
      "Epoch 44/50\n",
      "145/145 - 0s - loss: 0.2901 - accuracy: 0.9137 - val_loss: 1.3492 - val_accuracy: 0.5840\n",
      "Epoch 45/50\n",
      "145/145 - 0s - loss: 0.2749 - accuracy: 0.9215 - val_loss: 1.3748 - val_accuracy: 0.5827\n",
      "Epoch 46/50\n",
      "145/145 - 0s - loss: 0.2640 - accuracy: 0.9254 - val_loss: 1.4122 - val_accuracy: 0.5784\n",
      "Epoch 47/50\n",
      "145/145 - 0s - loss: 0.2605 - accuracy: 0.9247 - val_loss: 1.4377 - val_accuracy: 0.5774\n",
      "Epoch 48/50\n",
      "145/145 - 0s - loss: 0.2492 - accuracy: 0.9304 - val_loss: 1.4219 - val_accuracy: 0.5909\n",
      "Epoch 49/50\n",
      "145/145 - 0s - loss: 0.2353 - accuracy: 0.9356 - val_loss: 1.5072 - val_accuracy: 0.5733\n",
      "Epoch 50/50\n",
      "145/145 - 0s - loss: 0.2235 - accuracy: 0.9416 - val_loss: 1.5038 - val_accuracy: 0.5793\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "history = model.fit(\n",
    "    padded_training, y_train,\n",
    "    epochs= num_epochs,\n",
    "    validation_data = (padded_test, y_test),\n",
    "    verbose=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
